{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ConvNet PyTorch\n",
    "\n",
    "In this notebook, you'll learn how to use the powerful PyTorch framework to specify a conv net architecture and train it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.resnet import resnet18, resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "trans = T.Compose([\n",
    "        T.Scale(224),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=trans)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0),\n",
    "                         num_workers=0)\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=trans)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN), num_workers=0)\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,\n",
    "                          transform=trans)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "print_every = 50\n",
    "model = resnet18(num_classes=10).type(gpu_dtype)\n",
    "net = torch.nn.DataParallel(model)\n",
    "net.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=1e-3) # lr sets the learning rate of the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 50, loss = 2.1845\n",
      "batch time: 26.981170\n",
      "t = 100, loss = 1.8839\n",
      "batch time: 43.515702\n",
      "t = 150, loss = 1.6566\n",
      "batch time: 59.917748\n",
      "t = 200, loss = 2.0922\n",
      "batch time: 76.829442\n",
      "t = 250, loss = 1.7644\n",
      "batch time: 93.563796\n",
      "t = 300, loss = 1.5536\n",
      "batch time: 110.286716\n",
      "t = 350, loss = 1.4521\n",
      "batch time: 127.360156\n",
      "t = 400, loss = 1.3485\n",
      "batch time: 144.131982\n",
      "t = 450, loss = 1.5557\n",
      "batch time: 161.096243\n",
      "t = 500, loss = 1.3746\n",
      "batch time: 177.734682\n",
      "t = 550, loss = 1.5786\n",
      "batch time: 194.487471\n",
      "t = 600, loss = 1.3363\n",
      "batch time: 211.020145\n",
      "t = 650, loss = 1.6014\n",
      "batch time: 228.031575\n",
      "t = 700, loss = 1.7012\n",
      "batch time: 245.273895\n",
      "t = 750, loss = 1.2304\n",
      "batch time: 262.228501\n",
      "Checking accuracy on validation set\n",
      "Got 364 / 1000 correct (36.40)\n"
     ]
    }
   ],
   "source": [
    "def mytrain(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    import time\n",
    "    start_time =time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "                check_time = time.time()\n",
    "                print('batch time: %f' % (check_time - start_time))\n",
    "                # check_accuracy(model, loader_val)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "mytrain(net, loss_fn, optimizer, num_epochs=1)\n",
    "check_accuracy(net, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 3441 / 10000 correct (34.41)\n"
     ]
    }
   ],
   "source": [
    "best_model = net\n",
    "check_accuracy(best_model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "314 / 480.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
